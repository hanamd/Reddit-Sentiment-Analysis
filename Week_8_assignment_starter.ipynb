{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da97052f",
   "metadata": {},
   "source": [
    "# Week 8 assignment: NLP on social media data\n",
    "\n",
    "Take our social media we collected last week and:\n",
    "\n",
    "- extract the sentiment scores of the titles of the posts\n",
    "    - you can use the keyword method, Python packages, or other methods to extract the sentiment scores\n",
    "- plot a histogram of the sentiment scores\n",
    "- look at descriptive statistics (mean, median, standard deviation) of the sentiment scores\n",
    "- examine the text for some of the highest and lowest sentiment scores\n",
    "- write a short analysis of the results and our process, as well as propose one idea for something we could use this data for\n",
    "\n",
    "Optional advanced challenges:\n",
    "- Compare different sentiment analysis methods (e.g. textblob and VADER). Does one seem to work better than another?\n",
    "- Get the sentiments of the comments for each post. We can do a lot with this, such as:\n",
    "    - look at the average sentiment for each post and compare it with the sentiment of the title and/or text\n",
    "    - look at the distribution of sentiments for each post and find the posts with the widest range of sentiments (controversial posts)\n",
    "- Examine the subjectivity of our data (e.g. using textblob)\n",
    "- Use topic modeling on the posts\n",
    "    - you can also add in the comments to the topic model\n",
    "- Look at the most frequent words for positive and negative sentiment posts\n",
    "\n",
    "Note: There is no assignment solution file for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e84e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0ce430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Been living in Denver for a little over a year...</td>\n",
       "      <td>/r/denverfood/comments/1b6fom5/been_living_in_...</td>\n",
       "      <td>allanmuffins</td>\n",
       "      <td>174</td>\n",
       "      <td>You love Asian food OP haha. Only comment is h...</td>\n",
       "      <td>234</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue Pan Pizza</td>\n",
       "      <td>/r/denverfood/comments/1b62ip7/blue_pan_pizza/</td>\n",
       "      <td>JohnJAram</td>\n",
       "      <td>52</td>\n",
       "      <td>One of the owners played football at Michigan ...</td>\n",
       "      <td>103</td>\n",
       "      <td>Today was my son’s 19th birthday and based on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madras Cafe is closing this month. What's the ...</td>\n",
       "      <td>/r/denverfood/comments/1b6hgy7/madras_cafe_is_...</td>\n",
       "      <td>PlasmaWhore</td>\n",
       "      <td>9</td>\n",
       "      <td>Oh this is heartbreaking!  I don't know how an...</td>\n",
       "      <td>10</td>\n",
       "      <td>Just learned that Madras Cafe is closing. This...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was pretty cool to see:</td>\n",
       "      <td>/r/denverfood/comments/1b60ubw/this_was_pretty...</td>\n",
       "      <td>Namaste4Runner420</td>\n",
       "      <td>25</td>\n",
       "      <td>I guess it’s cool that Historian’s is finally ...</td>\n",
       "      <td>124</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slovenian sausage and/or deli?</td>\n",
       "      <td>/r/denverfood/comments/1b6hq3q/slovenian_sausa...</td>\n",
       "      <td>Likeabalrog</td>\n",
       "      <td>2</td>\n",
       "      <td>Cracovia in Westminster has a sausage sampler ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Are there any delis in the Denver area that se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Birthday Dinner Recommendation</td>\n",
       "      <td>/r/denverfood/comments/198cml9/birthday_dinner...</td>\n",
       "      <td>thesnowgirl147</td>\n",
       "      <td>12</td>\n",
       "      <td>I had my birthday dinner at a the Wolf’s Tailo...</td>\n",
       "      <td>0</td>\n",
       "      <td>I am looking to have a more chill birthday thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Is there a place that sells Vietnamese salt co...</td>\n",
       "      <td>/r/denverfood/comments/197l8qe/is_there_a_plac...</td>\n",
       "      <td>DougDimmadummy</td>\n",
       "      <td>11</td>\n",
       "      <td>I don’t know for sure if its what your looking...</td>\n",
       "      <td>18</td>\n",
       "      <td>Same as the title says. I’m looking for a plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Where to find Nduja</td>\n",
       "      <td>/r/denverfood/comments/197sb95/where_to_find_n...</td>\n",
       "      <td>broccoli15</td>\n",
       "      <td>5</td>\n",
       "      <td>Any well stocked gourmet market should have it...</td>\n",
       "      <td>7</td>\n",
       "      <td>Like the title says. Looking to find Nduja ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>What’s the best dumpling house in Denver/aurora?</td>\n",
       "      <td>/r/denverfood/comments/197dryi/whats_the_best_...</td>\n",
       "      <td>Mysterious-Monk4349</td>\n",
       "      <td>37</td>\n",
       "      <td>I love Nana's and the owner owns a few in Auro...</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>PSA for Denver vegans</td>\n",
       "      <td>/r/denverfood/comments/196w6tt/psa_for_denver_...</td>\n",
       "      <td>spicy_vegan69</td>\n",
       "      <td>302</td>\n",
       "      <td>Have any evidence or articles or is this a per...</td>\n",
       "      <td>337</td>\n",
       "      <td>Vegan Van is doing their 4th (or 5th?) Gofundm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Been living in Denver for a little over a year...   \n",
       "1                                       Blue Pan Pizza   \n",
       "2    Madras Cafe is closing this month. What's the ...   \n",
       "3                         This was pretty cool to see:   \n",
       "4                       Slovenian sausage and/or deli?   \n",
       "..                                                 ...   \n",
       "424                     Birthday Dinner Recommendation   \n",
       "425  Is there a place that sells Vietnamese salt co...   \n",
       "426                                Where to find Nduja   \n",
       "427   What’s the best dumpling house in Denver/aurora?   \n",
       "428                              PSA for Denver vegans   \n",
       "\n",
       "                                                  link               author  \\\n",
       "0    /r/denverfood/comments/1b6fom5/been_living_in_...         allanmuffins   \n",
       "1       /r/denverfood/comments/1b62ip7/blue_pan_pizza/            JohnJAram   \n",
       "2    /r/denverfood/comments/1b6hgy7/madras_cafe_is_...          PlasmaWhore   \n",
       "3    /r/denverfood/comments/1b60ubw/this_was_pretty...    Namaste4Runner420   \n",
       "4    /r/denverfood/comments/1b6hq3q/slovenian_sausa...          Likeabalrog   \n",
       "..                                                 ...                  ...   \n",
       "424  /r/denverfood/comments/198cml9/birthday_dinner...       thesnowgirl147   \n",
       "425  /r/denverfood/comments/197l8qe/is_there_a_plac...       DougDimmadummy   \n",
       "426  /r/denverfood/comments/197sb95/where_to_find_n...           broccoli15   \n",
       "427  /r/denverfood/comments/197dryi/whats_the_best_...  Mysterious-Monk4349   \n",
       "428  /r/denverfood/comments/196w6tt/psa_for_denver_...        spicy_vegan69   \n",
       "\n",
       "     total_comments                                           comments  score  \\\n",
       "0               174  You love Asian food OP haha. Only comment is h...    234   \n",
       "1                52  One of the owners played football at Michigan ...    103   \n",
       "2                 9  Oh this is heartbreaking!  I don't know how an...     10   \n",
       "3                25  I guess it’s cool that Historian’s is finally ...    124   \n",
       "4                 2  Cracovia in Westminster has a sausage sampler ...      5   \n",
       "..              ...                                                ...    ...   \n",
       "424              12  I had my birthday dinner at a the Wolf’s Tailo...      0   \n",
       "425              11  I don’t know for sure if its what your looking...     18   \n",
       "426               5  Any well stocked gourmet market should have it...      7   \n",
       "427              37  I love Nana's and the owner owns a few in Auro...     24   \n",
       "428             302  Have any evidence or articles or is this a per...    337   \n",
       "\n",
       "                                                  text  \n",
       "0                                                       \n",
       "1    Today was my son’s 19th birthday and based on ...  \n",
       "2    Just learned that Madras Cafe is closing. This...  \n",
       "3                                                       \n",
       "4    Are there any delis in the Denver area that se...  \n",
       "..                                                 ...  \n",
       "424  I am looking to have a more chill birthday thi...  \n",
       "425  Same as the title says. I’m looking for a plac...  \n",
       "426  Like the title says. Looking to find Nduja ide...  \n",
       "427                                                     \n",
       "428  Vegan Van is doing their 4th (or 5th?) Gofundm...  \n",
       "\n",
       "[429 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make connection\n",
    "con = sqlite3.connect(\"co_food_reddit.sqlite\")\n",
    "file = pd.read_sql_query('SELECT * FROM posts', con)\n",
    "\n",
    "#close connection\n",
    "con.close()\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357a0f5",
   "metadata": {},
   "source": [
    "## Method 1: keyword method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d22100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the sentiment scores of the titles of the posts\n",
    "\n",
    "sentiment_df = pd.read_csv('AFINN-en-165.txt', sep='\\t', names=['word','score'],index_col = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584c29c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandons</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abducted</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abduction</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yucky</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummy</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealot</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealots</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealous</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3382 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           score\n",
       "word            \n",
       "abandon       -2\n",
       "abandoned     -2\n",
       "abandons      -2\n",
       "abducted      -2\n",
       "abduction     -2\n",
       "...          ...\n",
       "yucky         -2\n",
       "yummy          3\n",
       "zealot        -2\n",
       "zealots       -2\n",
       "zealous        2\n",
       "\n",
       "[3382 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3cc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary format\n",
    "\n",
    "sentiment_dict = sentiment_df.to_dict()['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44a873e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abandon': -2,\n",
       " 'abandoned': -2,\n",
       " 'abandons': -2,\n",
       " 'abducted': -2,\n",
       " 'abduction': -2,\n",
       " 'abductions': -2,\n",
       " 'abhor': -3,\n",
       " 'abhorred': -3,\n",
       " 'abhorrent': -3,\n",
       " 'abhors': -3,\n",
       " 'abilities': 2,\n",
       " 'ability': 2,\n",
       " 'aboard': 1,\n",
       " 'aborted': -1,\n",
       " 'aborts': -1,\n",
       " 'absentee': -1,\n",
       " 'absentees': -1,\n",
       " 'absolve': 2,\n",
       " 'absolved': 2,\n",
       " 'absolves': 2,\n",
       " 'absolving': 2,\n",
       " 'absorbed': 1,\n",
       " 'abuse': -3,\n",
       " 'abused': -3,\n",
       " 'abuses': -3,\n",
       " 'abusing': -3,\n",
       " 'abusive': -3,\n",
       " 'accept': 1,\n",
       " 'acceptable': 1,\n",
       " 'acceptance': 1,\n",
       " 'accepted': 1,\n",
       " 'accepting': 1,\n",
       " 'accepts': 1,\n",
       " 'accessible': 1,\n",
       " 'accident': -2,\n",
       " 'accidental': -2,\n",
       " 'accidentally': -2,\n",
       " 'accidents': -2,\n",
       " 'acclaim': 2,\n",
       " 'acclaimed': 2,\n",
       " 'accolade': 2,\n",
       " 'accomplish': 2,\n",
       " 'accomplished': 2,\n",
       " 'accomplishes': 2,\n",
       " 'accomplishment': 2,\n",
       " 'accomplishments': 2,\n",
       " 'accusation': -2,\n",
       " 'accusations': -2,\n",
       " 'accuse': -2,\n",
       " 'accused': -2,\n",
       " 'accuses': -2,\n",
       " 'accusing': -2,\n",
       " 'ache': -2,\n",
       " 'achievable': 1,\n",
       " 'aching': -2,\n",
       " 'acquit': 2,\n",
       " 'acquits': 2,\n",
       " 'acquitted': 2,\n",
       " 'acquitting': 2,\n",
       " 'acrimonious': -3,\n",
       " 'active': 1,\n",
       " 'adequate': 1,\n",
       " 'admire': 3,\n",
       " 'admired': 3,\n",
       " 'admires': 3,\n",
       " 'admiring': 3,\n",
       " 'admit': -1,\n",
       " 'admits': -1,\n",
       " 'admitted': -1,\n",
       " 'admonish': -2,\n",
       " 'admonished': -2,\n",
       " 'adopt': 1,\n",
       " 'adopts': 1,\n",
       " 'adorable': 3,\n",
       " 'adoration': 3,\n",
       " 'adore': 3,\n",
       " 'adored': 3,\n",
       " 'adores': 3,\n",
       " 'adoring': 3,\n",
       " 'adoringly': 3,\n",
       " 'advanced': 1,\n",
       " 'advantage': 2,\n",
       " 'advantageous': 2,\n",
       " 'advantageously': 2,\n",
       " 'advantages': 2,\n",
       " 'adventure': 2,\n",
       " 'adventures': 2,\n",
       " 'adventurous': 2,\n",
       " 'adversary': -1,\n",
       " 'advisable': 1,\n",
       " 'affected': -1,\n",
       " 'affection': 3,\n",
       " 'affectionate': 3,\n",
       " 'affectionateness': 3,\n",
       " 'afflicted': -1,\n",
       " 'affordable': 2,\n",
       " 'affronted': -1,\n",
       " 'aficionados': 2,\n",
       " 'afraid': -2,\n",
       " 'aggravate': -2,\n",
       " 'aggravated': -2,\n",
       " 'aggravates': -2,\n",
       " 'aggravating': -2,\n",
       " 'aggression': -2,\n",
       " 'aggressions': -2,\n",
       " 'aggressive': -2,\n",
       " 'aggressiveness': -2,\n",
       " 'aghast': -2,\n",
       " 'agog': 2,\n",
       " 'agonise': -3,\n",
       " 'agonised': -3,\n",
       " 'agonises': -3,\n",
       " 'agonising': -3,\n",
       " 'agonize': -3,\n",
       " 'agonized': -3,\n",
       " 'agonizes': -3,\n",
       " 'agonizing': -3,\n",
       " 'agree': 1,\n",
       " 'agreeable': 2,\n",
       " 'agreed': 1,\n",
       " 'agreement': 1,\n",
       " 'agrees': 1,\n",
       " 'alarm': -2,\n",
       " 'alarmed': -2,\n",
       " 'alarmist': -2,\n",
       " 'alarmists': -2,\n",
       " 'alas': -1,\n",
       " 'alert': -1,\n",
       " 'alienation': -2,\n",
       " 'alive': 1,\n",
       " 'allegation': -2,\n",
       " 'allegations': -2,\n",
       " 'allergic': -2,\n",
       " 'allow': 1,\n",
       " 'ally': 2,\n",
       " 'alone': -2,\n",
       " 'altruistic': 2,\n",
       " 'amaze': 2,\n",
       " 'amazed': 2,\n",
       " 'amazes': 2,\n",
       " 'amazing': 4,\n",
       " 'ambitious': 2,\n",
       " 'ambivalent': -1,\n",
       " 'amicable': 2,\n",
       " 'amuse': 3,\n",
       " 'amused': 3,\n",
       " 'amusement': 3,\n",
       " 'amusements': 3,\n",
       " 'anger': -3,\n",
       " 'angered': -3,\n",
       " 'angers': -3,\n",
       " 'angry': -3,\n",
       " 'anguish': -3,\n",
       " 'anguished': -3,\n",
       " 'animosity': -2,\n",
       " 'annoy': -2,\n",
       " 'annoyance': -2,\n",
       " 'annoyed': -2,\n",
       " 'annoying': -2,\n",
       " 'annoys': -2,\n",
       " 'antagonistic': -2,\n",
       " 'anti': -1,\n",
       " 'anticipation': 1,\n",
       " 'anxiety': -2,\n",
       " 'anxious': -2,\n",
       " 'apathetic': -3,\n",
       " 'apathy': -3,\n",
       " 'apeshit': -3,\n",
       " 'apocalyptic': -2,\n",
       " 'apologise': -1,\n",
       " 'apologised': -1,\n",
       " 'apologises': -1,\n",
       " 'apologising': -1,\n",
       " 'apologize': -1,\n",
       " 'apologized': -1,\n",
       " 'apologizes': -1,\n",
       " 'apologizing': -1,\n",
       " 'apology': -1,\n",
       " 'appalled': -2,\n",
       " 'appalling': -2,\n",
       " 'appealing': 2,\n",
       " 'appease': 2,\n",
       " 'appeased': 2,\n",
       " 'appeases': 2,\n",
       " 'appeasing': 2,\n",
       " 'applaud': 2,\n",
       " 'applauded': 2,\n",
       " 'applauding': 2,\n",
       " 'applauds': 2,\n",
       " 'applause': 2,\n",
       " 'appreciate': 2,\n",
       " 'appreciated': 2,\n",
       " 'appreciates': 2,\n",
       " 'appreciating': 2,\n",
       " 'appreciation': 2,\n",
       " 'apprehensive': -2,\n",
       " 'appropriate': 2,\n",
       " 'appropriately': 2,\n",
       " 'approval': 2,\n",
       " 'approved': 2,\n",
       " 'approves': 2,\n",
       " 'ardent': 1,\n",
       " 'arrest': -2,\n",
       " 'arrested': -3,\n",
       " 'arrests': -2,\n",
       " 'arrogant': -2,\n",
       " 'arsehole': -4,\n",
       " 'ashame': -2,\n",
       " 'ashamed': -2,\n",
       " 'ass': -4,\n",
       " 'assassination': -3,\n",
       " 'assassinations': -3,\n",
       " 'assault': -2,\n",
       " 'assaults': -2,\n",
       " 'asset': 2,\n",
       " 'assets': 2,\n",
       " 'assfucking': -4,\n",
       " 'asshole': -4,\n",
       " 'astonished': 2,\n",
       " 'astound': 3,\n",
       " 'astounded': 3,\n",
       " 'astounding': 3,\n",
       " 'astoundingly': 3,\n",
       " 'astounds': 3,\n",
       " 'atrocious': -3,\n",
       " 'atrocity': -3,\n",
       " 'attack': -1,\n",
       " 'attacked': -1,\n",
       " 'attacking': -1,\n",
       " 'attacks': -1,\n",
       " 'attract': 1,\n",
       " 'attracted': 1,\n",
       " 'attracting': 2,\n",
       " 'attraction': 2,\n",
       " 'attractions': 2,\n",
       " 'attractive': 2,\n",
       " 'attractively': 2,\n",
       " 'attractiveness': 2,\n",
       " 'attracts': 1,\n",
       " 'audacious': 3,\n",
       " 'aura': 1,\n",
       " 'authority': 1,\n",
       " 'avenge': -2,\n",
       " 'avenged': -2,\n",
       " 'avenger': -2,\n",
       " 'avengers': -2,\n",
       " 'avenges': -2,\n",
       " 'avenging': -2,\n",
       " 'avert': -1,\n",
       " 'averted': -1,\n",
       " 'averts': -1,\n",
       " 'avid': 2,\n",
       " 'avoid': -1,\n",
       " 'avoided': -1,\n",
       " 'avoids': -1,\n",
       " 'await': -1,\n",
       " 'awaited': -1,\n",
       " 'awaits': -1,\n",
       " 'award': 3,\n",
       " 'awarded': 3,\n",
       " 'awards': 3,\n",
       " 'awesome': 4,\n",
       " 'awful': -3,\n",
       " 'awkward': -2,\n",
       " 'axe': -1,\n",
       " 'axed': -1,\n",
       " 'backed': 1,\n",
       " 'backing': 2,\n",
       " 'backs': 1,\n",
       " 'bad': -3,\n",
       " 'bad luck': -2,\n",
       " 'badass': -3,\n",
       " 'badly': -3,\n",
       " 'badness': -3,\n",
       " 'bailout': -2,\n",
       " 'balanced': 1,\n",
       " 'bamboozle': -2,\n",
       " 'bamboozled': -2,\n",
       " 'bamboozles': -2,\n",
       " 'ban': -2,\n",
       " 'banish': -1,\n",
       " 'bankrupt': -3,\n",
       " 'bankruptcy': -3,\n",
       " 'bankster': -3,\n",
       " 'banned': -2,\n",
       " 'barbarian': -2,\n",
       " 'barbaric': -2,\n",
       " 'barbarous': -2,\n",
       " 'bargain': 2,\n",
       " 'barrier': -2,\n",
       " 'bastard': -5,\n",
       " 'bastards': -5,\n",
       " 'battle': -1,\n",
       " 'battled': -1,\n",
       " 'battles': -1,\n",
       " 'battling': -2,\n",
       " 'beaten': -2,\n",
       " 'beatific': 3,\n",
       " 'beating': -1,\n",
       " 'beauties': 3,\n",
       " 'beautiful': 3,\n",
       " 'beautifully': 3,\n",
       " 'beautify': 3,\n",
       " 'beauty': 3,\n",
       " 'befit': 2,\n",
       " 'befitting': 2,\n",
       " 'belittle': -2,\n",
       " 'belittled': -2,\n",
       " 'beloved': 3,\n",
       " 'benefactor': 2,\n",
       " 'benefactors': 2,\n",
       " 'benefit': 2,\n",
       " 'benefits': 2,\n",
       " 'benefitted': 2,\n",
       " 'benefitting': 2,\n",
       " 'benevolent': 3,\n",
       " 'bereave': -2,\n",
       " 'bereaved': -2,\n",
       " 'bereaves': -2,\n",
       " 'bereaving': -2,\n",
       " 'best': 3,\n",
       " 'best damn': 4,\n",
       " 'betray': -3,\n",
       " 'betrayal': -3,\n",
       " 'betrayed': -3,\n",
       " 'betraying': -3,\n",
       " 'betrays': -3,\n",
       " 'better': 2,\n",
       " 'bias': -1,\n",
       " 'biased': -2,\n",
       " 'big': 1,\n",
       " 'bitch': -5,\n",
       " 'bitches': -5,\n",
       " 'bitter': -2,\n",
       " 'bitterest': -2,\n",
       " 'bitterly': -2,\n",
       " 'bizarre': -2,\n",
       " 'blackmail': -3,\n",
       " 'blackmailed': -3,\n",
       " 'blackmailing': -3,\n",
       " 'blackmails': -3,\n",
       " 'blah': -2,\n",
       " 'blame': -2,\n",
       " 'blamed': -2,\n",
       " 'blames': -2,\n",
       " 'blaming': -2,\n",
       " 'bless': 2,\n",
       " 'blesses': 2,\n",
       " 'blessing': 3,\n",
       " 'blessings': 3,\n",
       " 'blind': -1,\n",
       " 'bliss': 3,\n",
       " 'blissful': 3,\n",
       " 'blithe': 2,\n",
       " 'bloated': -1,\n",
       " 'block': -1,\n",
       " 'blockade': -2,\n",
       " 'blockbuster': 3,\n",
       " 'blocked': -1,\n",
       " 'blocking': -1,\n",
       " 'blocks': -1,\n",
       " 'bloody': -3,\n",
       " 'blurry': -2,\n",
       " 'boastful': -2,\n",
       " 'bold': 2,\n",
       " 'boldly': 2,\n",
       " 'bomb': -1,\n",
       " 'boost': 1,\n",
       " 'boosted': 1,\n",
       " 'boosting': 1,\n",
       " 'boosts': 1,\n",
       " 'bore': -2,\n",
       " 'bored': -2,\n",
       " 'boring': -3,\n",
       " 'bother': -2,\n",
       " 'bothered': -2,\n",
       " 'bothers': -2,\n",
       " 'bothersome': -2,\n",
       " 'boycott': -2,\n",
       " 'boycotted': -2,\n",
       " 'boycotting': -2,\n",
       " 'boycotts': -2,\n",
       " 'brainwashing': -3,\n",
       " 'brave': 2,\n",
       " 'braveness': 2,\n",
       " 'bravery': 2,\n",
       " 'bravura': 3,\n",
       " 'breach': -2,\n",
       " 'breached': -2,\n",
       " 'breaches': -2,\n",
       " 'breaching': -2,\n",
       " 'breakthrough': 3,\n",
       " 'breathtaking': 5,\n",
       " 'bribe': -3,\n",
       " 'bribed': -3,\n",
       " 'bribes': -3,\n",
       " 'bribing': -3,\n",
       " 'bright': 1,\n",
       " 'brightest': 2,\n",
       " 'brightness': 1,\n",
       " 'brilliant': 4,\n",
       " 'brilliance': 3,\n",
       " 'brilliances': 3,\n",
       " 'brisk': 2,\n",
       " 'broke': -1,\n",
       " 'broken': -1,\n",
       " 'brooding': -2,\n",
       " 'brutal': -3,\n",
       " 'brutally': -3,\n",
       " 'bullied': -2,\n",
       " 'bullshit': -4,\n",
       " 'bully': -2,\n",
       " 'bullying': -2,\n",
       " 'bummer': -2,\n",
       " 'buoyant': 2,\n",
       " 'burden': -2,\n",
       " 'burdened': -2,\n",
       " 'burdening': -2,\n",
       " 'burdens': -2,\n",
       " 'burglar': -2,\n",
       " 'burglary': -2,\n",
       " 'calm': 2,\n",
       " 'calmed': 2,\n",
       " 'calming': 2,\n",
       " 'calms': 2,\n",
       " \"can't stand\": -3,\n",
       " 'cancel': -1,\n",
       " 'cancelled': -1,\n",
       " 'cancelling': -1,\n",
       " 'cancels': -1,\n",
       " 'cancer': -1,\n",
       " 'capabilities': 1,\n",
       " 'capability': 1,\n",
       " 'capable': 1,\n",
       " 'captivated': 3,\n",
       " 'care': 2,\n",
       " 'carefree': 1,\n",
       " 'careful': 2,\n",
       " 'carefully': 2,\n",
       " 'carefulness': 2,\n",
       " 'careless': -2,\n",
       " 'cares': 2,\n",
       " 'caring': 2,\n",
       " 'cashing in': -2,\n",
       " 'casualty': -2,\n",
       " 'catastrophe': -3,\n",
       " 'catastrophic': -4,\n",
       " 'cautious': -1,\n",
       " 'celebrate': 3,\n",
       " 'celebrated': 3,\n",
       " 'celebrates': 3,\n",
       " 'celebrating': 3,\n",
       " 'celebration': 3,\n",
       " 'celebrations': 3,\n",
       " 'censor': -2,\n",
       " 'censored': -2,\n",
       " 'censors': -2,\n",
       " 'certain': 1,\n",
       " 'chagrin': -2,\n",
       " 'chagrined': -2,\n",
       " 'challenge': -1,\n",
       " 'champion': 2,\n",
       " 'championed': 2,\n",
       " 'champions': 2,\n",
       " 'chance': 2,\n",
       " 'chances': 2,\n",
       " 'chaos': -2,\n",
       " 'chaotic': -2,\n",
       " 'charged': -3,\n",
       " 'charges': -2,\n",
       " 'charisma': 2,\n",
       " 'charitable': 2,\n",
       " 'charm': 3,\n",
       " 'charming': 3,\n",
       " 'charmingly': 3,\n",
       " 'charmless': -3,\n",
       " 'chastise': -3,\n",
       " 'chastised': -3,\n",
       " 'chastises': -3,\n",
       " 'chastising': -3,\n",
       " 'cheat': -3,\n",
       " 'cheated': -3,\n",
       " 'cheater': -3,\n",
       " 'cheaters': -3,\n",
       " 'cheating': -3,\n",
       " 'cheats': -3,\n",
       " 'cheer': 2,\n",
       " 'cheered': 2,\n",
       " 'cheerful': 2,\n",
       " 'cheerfully': 2,\n",
       " 'cheering': 2,\n",
       " 'cheerless': -2,\n",
       " 'cheers': 2,\n",
       " 'cheery': 3,\n",
       " 'cherish': 2,\n",
       " 'cherished': 2,\n",
       " 'cherishes': 2,\n",
       " 'cherishing': 2,\n",
       " 'chic': 2,\n",
       " 'chide': -3,\n",
       " 'chided': -3,\n",
       " 'chides': -3,\n",
       " 'chiding': -3,\n",
       " 'childish': -2,\n",
       " 'chilling': -1,\n",
       " 'choke': -2,\n",
       " 'choked': -2,\n",
       " 'chokes': -2,\n",
       " 'choking': -2,\n",
       " 'clarifies': 2,\n",
       " 'clarity': 2,\n",
       " 'clash': -2,\n",
       " 'classy': 3,\n",
       " 'clean': 2,\n",
       " 'cleaner': 2,\n",
       " 'clear': 1,\n",
       " 'cleared': 1,\n",
       " 'clearly': 1,\n",
       " 'clears': 1,\n",
       " 'clever': 2,\n",
       " 'clouded': -1,\n",
       " 'clueless': -2,\n",
       " 'cock': -5,\n",
       " 'cocksucker': -5,\n",
       " 'cocksuckers': -5,\n",
       " 'cocky': -2,\n",
       " 'coerced': -2,\n",
       " 'coercion': -2,\n",
       " 'collapse': -2,\n",
       " 'collapsed': -2,\n",
       " 'collapses': -2,\n",
       " 'collapsing': -2,\n",
       " 'collide': -1,\n",
       " 'collides': -1,\n",
       " 'colliding': -1,\n",
       " 'collision': -2,\n",
       " 'collisions': -2,\n",
       " 'colluding': -3,\n",
       " 'combat': -1,\n",
       " 'combats': -1,\n",
       " 'comedy': 1,\n",
       " 'comfort': 2,\n",
       " 'comfortable': 2,\n",
       " 'comfortably': 2,\n",
       " 'comforting': 2,\n",
       " 'comforts': 2,\n",
       " 'comic': 1,\n",
       " 'commend': 2,\n",
       " 'commended': 2,\n",
       " 'commit': 1,\n",
       " 'commitment': 2,\n",
       " 'commits': 1,\n",
       " 'committed': 1,\n",
       " 'committing': 1,\n",
       " 'compassion': 2,\n",
       " 'compassionate': 2,\n",
       " 'compelled': 1,\n",
       " 'competencies': 1,\n",
       " 'competent': 2,\n",
       " 'competitive': 2,\n",
       " 'complacent': -2,\n",
       " 'complain': -2,\n",
       " 'complained': -2,\n",
       " 'complaining': -2,\n",
       " 'complains': -2,\n",
       " 'complaint': -2,\n",
       " 'complaints': -2,\n",
       " 'complicating': -2,\n",
       " 'compliment': 2,\n",
       " 'complimented': 2,\n",
       " 'compliments': 2,\n",
       " 'comprehensive': 2,\n",
       " 'concerned': -2,\n",
       " 'conciliate': 2,\n",
       " 'conciliated': 2,\n",
       " 'conciliates': 2,\n",
       " 'conciliating': 2,\n",
       " 'condemn': -2,\n",
       " 'condemnation': -2,\n",
       " 'condemned': -2,\n",
       " 'condemns': -2,\n",
       " 'confidence': 2,\n",
       " 'confident': 2,\n",
       " 'confidently': 2,\n",
       " 'conflict': -2,\n",
       " 'conflicting': -2,\n",
       " 'conflictive': -2,\n",
       " 'conflicts': -2,\n",
       " 'confuse': -2,\n",
       " 'confused': -2,\n",
       " 'confusing': -2,\n",
       " 'congrats': 2,\n",
       " 'congratulate': 2,\n",
       " 'congratulation': 2,\n",
       " 'congratulations': 2,\n",
       " 'consent': 2,\n",
       " 'consents': 2,\n",
       " 'consolable': 2,\n",
       " 'conspiracy': -3,\n",
       " 'constipation': -2,\n",
       " 'constrained': -2,\n",
       " 'contagion': -2,\n",
       " 'contagions': -2,\n",
       " 'contagious': -1,\n",
       " 'contaminant': -2,\n",
       " 'contaminants': -2,\n",
       " 'contaminate': -2,\n",
       " 'contaminated': -2,\n",
       " 'contaminates': -2,\n",
       " 'contaminating': -2,\n",
       " 'contamination': -2,\n",
       " 'contaminations': -2,\n",
       " 'contempt': -2,\n",
       " 'contemptible': -2,\n",
       " 'contemptuous': -2,\n",
       " 'contemptuously': -2,\n",
       " 'contend': -1,\n",
       " 'contender': -1,\n",
       " 'contending': -1,\n",
       " 'contentious': -2,\n",
       " 'contestable': -2,\n",
       " 'controversial': -2,\n",
       " 'controversially': -2,\n",
       " 'controversies': -2,\n",
       " 'controversy': -2,\n",
       " 'convicted': -2,\n",
       " 'convince': 1,\n",
       " 'convinced': 1,\n",
       " 'convinces': 1,\n",
       " 'convivial': 2,\n",
       " 'cool': 1,\n",
       " 'cool stuff': 3,\n",
       " 'cornered': -2,\n",
       " 'corpse': -1,\n",
       " 'corrupt': -3,\n",
       " 'corrupted': -3,\n",
       " 'corrupting': -3,\n",
       " 'corruption': -3,\n",
       " 'corrupts': -3,\n",
       " 'costly': -2,\n",
       " 'courage': 2,\n",
       " 'courageous': 2,\n",
       " 'courageously': 2,\n",
       " 'courageousness': 2,\n",
       " 'courteous': 2,\n",
       " 'courtesy': 2,\n",
       " 'cover-up': -3,\n",
       " 'coward': -2,\n",
       " 'cowardly': -2,\n",
       " 'coziness': 2,\n",
       " 'cramp': -1,\n",
       " 'crap': -3,\n",
       " 'crappy': -3,\n",
       " 'crash': -2,\n",
       " 'crazier': -2,\n",
       " 'craziest': -2,\n",
       " 'crazy': -2,\n",
       " 'creative': 2,\n",
       " 'crestfallen': -2,\n",
       " 'cried': -2,\n",
       " 'cries': -2,\n",
       " 'crime': -3,\n",
       " 'crimes': -3,\n",
       " 'criminal': -3,\n",
       " 'criminals': -3,\n",
       " 'criminate': -3,\n",
       " 'criminated': -3,\n",
       " 'criminates': -3,\n",
       " 'crisis': -3,\n",
       " 'critic': -2,\n",
       " 'criticise': -2,\n",
       " 'criticised': -2,\n",
       " 'criticises': -2,\n",
       " 'criticising': -2,\n",
       " 'criticism': -2,\n",
       " 'criticize': -2,\n",
       " 'criticized': -2,\n",
       " 'criticizes': -2,\n",
       " 'criticizing': -2,\n",
       " 'critics': -2,\n",
       " 'critique': -2,\n",
       " 'crowding': -1,\n",
       " 'crude': -1,\n",
       " 'cruel': -3,\n",
       " 'cruelty': -3,\n",
       " 'crush': -1,\n",
       " 'crushed': -2,\n",
       " 'crushes': -1,\n",
       " 'crushing': -1,\n",
       " 'cry': -1,\n",
       " 'crying': -2,\n",
       " 'cunning': 2,\n",
       " 'cunt': -5,\n",
       " 'curious': 1,\n",
       " 'curse': -1,\n",
       " 'cut': -1,\n",
       " 'cutback': -2,\n",
       " 'cutbacks': -2,\n",
       " 'cute': 2,\n",
       " 'cuts': -1,\n",
       " 'cutting': -1,\n",
       " 'cynic': -2,\n",
       " 'cynical': -2,\n",
       " 'cynicism': -2,\n",
       " 'damage': -3,\n",
       " 'damaged': -3,\n",
       " 'damages': -3,\n",
       " 'damaging': -3,\n",
       " 'damn': -2,\n",
       " 'damn cute': 3,\n",
       " 'damn good': 4,\n",
       " 'damned': -4,\n",
       " 'damnit': -4,\n",
       " 'danger': -2,\n",
       " 'dangerous': -2,\n",
       " 'dangerously': -2,\n",
       " 'daredevil': 2,\n",
       " 'daring': 2,\n",
       " 'darkest': -2,\n",
       " 'darkness': -1,\n",
       " 'dauntless': 2,\n",
       " 'dazzling': 3,\n",
       " 'dead': -3,\n",
       " 'deadening': -2,\n",
       " 'deadlock': -2,\n",
       " 'deadly': -3,\n",
       " 'deafening': -1,\n",
       " 'dear': 2,\n",
       " 'dearly': 3,\n",
       " 'death': -2,\n",
       " 'deaths': -2,\n",
       " 'debonair': 2,\n",
       " 'debt': -2,\n",
       " 'deceit': -3,\n",
       " 'deceitful': -3,\n",
       " 'deceive': -3,\n",
       " 'deceived': -3,\n",
       " 'deceives': -3,\n",
       " 'deceiving': -3,\n",
       " 'deception': -3,\n",
       " 'deceptive': -3,\n",
       " 'decisive': 1,\n",
       " 'dedicated': 2,\n",
       " 'dedication': 2,\n",
       " 'defeat': -2,\n",
       " 'defeated': -2,\n",
       " 'defect': -3,\n",
       " 'defective': -3,\n",
       " 'defects': -3,\n",
       " 'defender': 2,\n",
       " 'defenders': 2,\n",
       " 'defenseless': -2,\n",
       " 'defer': -1,\n",
       " 'deferring': -1,\n",
       " 'defiant': -1,\n",
       " 'deficient': -2,\n",
       " 'deficiency': -2,\n",
       " 'deficiencies': -2,\n",
       " 'deficit': -2,\n",
       " 'deformed': -2,\n",
       " 'deformities': -2,\n",
       " 'deformity': -2,\n",
       " 'defraud': -3,\n",
       " 'defrauds': -3,\n",
       " 'deft': 2,\n",
       " 'defunct': -2,\n",
       " 'degrade': -2,\n",
       " 'degraded': -2,\n",
       " 'degrades': -2,\n",
       " 'dehumanize': -2,\n",
       " 'dehumanized': -2,\n",
       " 'dehumanizes': -2,\n",
       " 'dehumanizing': -2,\n",
       " 'deject': -2,\n",
       " 'dejected': -2,\n",
       " 'dejecting': -2,\n",
       " 'dejects': -2,\n",
       " 'delay': -1,\n",
       " 'delayed': -1,\n",
       " 'delectable': 3,\n",
       " 'delicious': 3,\n",
       " 'delight': 3,\n",
       " 'delighted': 3,\n",
       " 'delightful': 3,\n",
       " 'delightfully': 3,\n",
       " 'delighting': 3,\n",
       " 'delights': 3,\n",
       " 'demand': -1,\n",
       " 'demanded': -1,\n",
       " 'demanding': -1,\n",
       " 'demands': -1,\n",
       " 'demonstration': -1,\n",
       " 'demoralize': -2,\n",
       " 'demoralized': -2,\n",
       " 'demoralizes': -2,\n",
       " 'demoralizing': -2,\n",
       " 'denial': -2,\n",
       " 'denials': -2,\n",
       " 'denied': -2,\n",
       " 'denier': -2,\n",
       " 'deniers': -2,\n",
       " 'denies': -2,\n",
       " 'denounce': -2,\n",
       " 'denounces': -2,\n",
       " 'dent': -2,\n",
       " 'deny': -2,\n",
       " 'denying': -2,\n",
       " 'deplore': -3,\n",
       " 'deplored': -3,\n",
       " 'deplores': -3,\n",
       " 'deploring': -3,\n",
       " 'deport': -2,\n",
       " 'deported': -2,\n",
       " 'deporting': -2,\n",
       " 'deports': -2,\n",
       " 'deportation': -2,\n",
       " 'deportations': -2,\n",
       " 'depressed': -2,\n",
       " 'depressing': -2,\n",
       " 'deprivation': -3,\n",
       " 'derail': -2,\n",
       " 'derailed': -2,\n",
       " 'derails': -2,\n",
       " 'derelict': -2,\n",
       " 'deride': -2,\n",
       " 'derided': -2,\n",
       " 'derides': -2,\n",
       " 'deriding': -2,\n",
       " 'derision': -2,\n",
       " 'desirable': 2,\n",
       " 'desire': 1,\n",
       " 'desired': 2,\n",
       " 'desirous': 2,\n",
       " 'despair': -3,\n",
       " 'despairing': -3,\n",
       " 'despairs': -3,\n",
       " 'desperate': -3,\n",
       " 'desperately': -3,\n",
       " 'despondent': -3,\n",
       " 'destroy': -3,\n",
       " 'destroyed': -3,\n",
       " 'destroying': -3,\n",
       " 'destroys': -3,\n",
       " 'destruction': -3,\n",
       " 'destructive': -3,\n",
       " 'detached': -1,\n",
       " 'detain': -2,\n",
       " 'detained': -2,\n",
       " 'detention': -2,\n",
       " 'deteriorate': -2,\n",
       " 'deteriorated': -2,\n",
       " 'deteriorates': -2,\n",
       " 'deteriorating': -2,\n",
       " 'determined': 2,\n",
       " 'deterrent': -2,\n",
       " 'detract': -1,\n",
       " 'detracted': -1,\n",
       " 'detracts': -1,\n",
       " 'devastate': -2,\n",
       " 'devastated': -2,\n",
       " 'devastating': -2,\n",
       " 'devastation': -2,\n",
       " 'devastations': -2,\n",
       " 'devoted': 3,\n",
       " 'devotion': 2,\n",
       " 'devotional': 2,\n",
       " 'diamond': 1,\n",
       " 'dick': -4,\n",
       " 'dickhead': -4,\n",
       " 'die': -3,\n",
       " 'died': -3,\n",
       " 'difficult': -1,\n",
       " 'diffident': -2,\n",
       " 'dignity': 2,\n",
       " 'dilemma': -1,\n",
       " 'dilligence': 2,\n",
       " 'dipshit': -3,\n",
       " 'dire': -3,\n",
       " 'direful': -3,\n",
       " 'dirt': -2,\n",
       " 'dirtier': -2,\n",
       " 'dirtiest': -2,\n",
       " 'dirty': -2,\n",
       " 'disabilities': -2,\n",
       " 'disability': -2,\n",
       " 'disabling': -1,\n",
       " 'disadvantage': -2,\n",
       " 'disadvantaged': -2,\n",
       " 'disagree': -2,\n",
       " 'disagreeable': -2,\n",
       " 'disagreement': -2,\n",
       " 'disappear': -1,\n",
       " 'disappeared': -1,\n",
       " 'disappears': -1,\n",
       " 'disappoint': -2,\n",
       " 'disappointed': -2,\n",
       " 'disappointing': -2,\n",
       " 'disappointment': -2,\n",
       " 'disappointments': -2,\n",
       " 'disappoints': -2,\n",
       " 'disapproval': -2,\n",
       " 'disapprovals': -2,\n",
       " 'disapprove': -2,\n",
       " 'disapproved': -2,\n",
       " 'disapproves': -2,\n",
       " 'disapproving': -2,\n",
       " 'disaster': -2,\n",
       " 'disasters': -2,\n",
       " 'disastrous': -3,\n",
       " 'disbelieve': -2,\n",
       " 'discard': -1,\n",
       " 'discarded': -1,\n",
       " 'discarding': -1,\n",
       " 'discards': -1,\n",
       " 'discernment': 2,\n",
       " 'discomfort': -2,\n",
       " 'disconsolate': -2,\n",
       " 'disconsolation': -2,\n",
       " 'discontented': -2,\n",
       " 'discord': -2,\n",
       " 'discounted': -1,\n",
       " 'discouraged': -2,\n",
       " 'discredited': -2,\n",
       " 'discriminate': -2,\n",
       " 'discriminated': -2,\n",
       " 'discriminates': -2,\n",
       " 'discriminating': -2,\n",
       " 'discriminatory': -2,\n",
       " 'disdain': -2,\n",
       " 'disease': -1,\n",
       " 'diseases': -1,\n",
       " 'disgrace': -2,\n",
       " 'disgraced': -2,\n",
       " 'disguise': -1,\n",
       " 'disguised': -1,\n",
       " 'disguises': -1,\n",
       " 'disguising': -1,\n",
       " 'disgust': -3,\n",
       " 'disgusted': -3,\n",
       " 'disgustful': -3,\n",
       " 'disgusting': -3,\n",
       " 'disheartened': -2,\n",
       " 'dishonest': -2,\n",
       " 'disillusioned': -2,\n",
       " 'disinclined': -2,\n",
       " 'disjointed': -2,\n",
       " 'dislike': -2,\n",
       " 'disliked': -2,\n",
       " 'dislikes': -2,\n",
       " 'dismal': -2,\n",
       " 'dismayed': -2,\n",
       " 'dismissed': -2,\n",
       " 'disorder': -2,\n",
       " 'disorders': -2,\n",
       " 'disorganized': -2,\n",
       " 'disoriented': -2,\n",
       " 'disparage': -2,\n",
       " 'disparaged': -2,\n",
       " 'disparages': -2,\n",
       " 'disparaging': -2,\n",
       " 'displeased': -2,\n",
       " 'displeasure': -2,\n",
       " 'disproportionate': -2,\n",
       " 'dispute': -2,\n",
       " 'disputed': -2,\n",
       " 'disputes': -2,\n",
       " 'disputing': -2,\n",
       " 'disqualified': -2,\n",
       " 'disquiet': -2,\n",
       " 'disregard': -2,\n",
       " 'disregarded': -2,\n",
       " 'disregarding': -2,\n",
       " 'disregards': -2,\n",
       " 'disrespect': -2,\n",
       " 'disrespected': -2,\n",
       " 'disrupt': -2,\n",
       " 'disrupted': -2,\n",
       " 'disrupting': -2,\n",
       " 'disruption': -2,\n",
       " 'disruptions': -2,\n",
       " 'disruptive': -2,\n",
       " 'disrupts': -2,\n",
       " 'dissatisfied': -2,\n",
       " 'distasteful': -2,\n",
       " 'distinguished': 2,\n",
       " 'distort': -2,\n",
       " 'distorted': -2,\n",
       " 'distorting': -2,\n",
       " 'distorts': -2,\n",
       " 'distract': -2,\n",
       " 'distracted': -2,\n",
       " 'distraction': -2,\n",
       " 'distracts': -2,\n",
       " 'distress': -2,\n",
       " 'distressed': -2,\n",
       " 'distresses': -2,\n",
       " 'distressing': -2,\n",
       " 'distrust': -3,\n",
       " 'distrustful': -3,\n",
       " 'disturb': -2,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4d00e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "title_sentiments = []\n",
    "\n",
    "# for loop to get sentimental value of each word in title \n",
    "for title in file['title']:\n",
    "    words = title.lower().split()\n",
    "    this_title_sentiments = []\n",
    "    for w in words:\n",
    "        if w in sentiment_dict.keys():\n",
    "            this_title_sentiments.append(sentiment_dict[w])\n",
    "        else:\n",
    "            this_title_sentiments.append(0)\n",
    "    title_sentiments.append(np.mean(this_title_sentiments))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d61c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column in downloaded file\n",
    "file['keyword_sentiment'] = title_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1138a87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Been living in Denver for a little over a year...</td>\n",
       "      <td>/r/denverfood/comments/1b6fom5/been_living_in_...</td>\n",
       "      <td>allanmuffins</td>\n",
       "      <td>174</td>\n",
       "      <td>You love Asian food OP haha. Only comment is h...</td>\n",
       "      <td>234</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue Pan Pizza</td>\n",
       "      <td>/r/denverfood/comments/1b62ip7/blue_pan_pizza/</td>\n",
       "      <td>JohnJAram</td>\n",
       "      <td>52</td>\n",
       "      <td>One of the owners played football at Michigan ...</td>\n",
       "      <td>103</td>\n",
       "      <td>Today was my son’s 19th birthday and based on ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madras Cafe is closing this month. What's the ...</td>\n",
       "      <td>/r/denverfood/comments/1b6hgy7/madras_cafe_is_...</td>\n",
       "      <td>PlasmaWhore</td>\n",
       "      <td>9</td>\n",
       "      <td>Oh this is heartbreaking!  I don't know how an...</td>\n",
       "      <td>10</td>\n",
       "      <td>Just learned that Madras Cafe is closing. This...</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was pretty cool to see:</td>\n",
       "      <td>/r/denverfood/comments/1b60ubw/this_was_pretty...</td>\n",
       "      <td>Namaste4Runner420</td>\n",
       "      <td>25</td>\n",
       "      <td>I guess it’s cool that Historian’s is finally ...</td>\n",
       "      <td>124</td>\n",
       "      <td></td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slovenian sausage and/or deli?</td>\n",
       "      <td>/r/denverfood/comments/1b6hq3q/slovenian_sausa...</td>\n",
       "      <td>Likeabalrog</td>\n",
       "      <td>2</td>\n",
       "      <td>Cracovia in Westminster has a sausage sampler ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Are there any delis in the Denver area that se...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Been living in Denver for a little over a year...   \n",
       "1                                     Blue Pan Pizza   \n",
       "2  Madras Cafe is closing this month. What's the ...   \n",
       "3                       This was pretty cool to see:   \n",
       "4                     Slovenian sausage and/or deli?   \n",
       "\n",
       "                                                link             author  \\\n",
       "0  /r/denverfood/comments/1b6fom5/been_living_in_...       allanmuffins   \n",
       "1     /r/denverfood/comments/1b62ip7/blue_pan_pizza/          JohnJAram   \n",
       "2  /r/denverfood/comments/1b6hgy7/madras_cafe_is_...        PlasmaWhore   \n",
       "3  /r/denverfood/comments/1b60ubw/this_was_pretty...  Namaste4Runner420   \n",
       "4  /r/denverfood/comments/1b6hq3q/slovenian_sausa...        Likeabalrog   \n",
       "\n",
       "   total_comments                                           comments  score  \\\n",
       "0             174  You love Asian food OP haha. Only comment is h...    234   \n",
       "1              52  One of the owners played football at Michigan ...    103   \n",
       "2               9  Oh this is heartbreaking!  I don't know how an...     10   \n",
       "3              25  I guess it’s cool that Historian’s is finally ...    124   \n",
       "4               2  Cracovia in Westminster has a sausage sampler ...      5   \n",
       "\n",
       "                                                text  keyword_sentiment  \n",
       "0                                                              0.000000  \n",
       "1  Today was my son’s 19th birthday and based on ...           0.000000  \n",
       "2  Just learned that Madras Cafe is closing. This...           0.230769  \n",
       "3                                                              0.333333  \n",
       "4  Are there any delis in the Denver area that se...           0.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fbde47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file['keyword_sentiment'].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c03f4a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    429.000000\n",
       "mean       0.112995\n",
       "std        0.294922\n",
       "min       -2.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.166667\n",
       "max        1.500000\n",
       "Name: keyword_sentiment, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['keyword_sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4db9b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keyword_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Worst Experience at Lo Stella</td>\n",
       "      <td>-0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>Restaurant's Added Fees are Ridiculous</td>\n",
       "      <td>-0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Chipotle or Illegal Pete’s</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Missing Donuts</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>Stuck</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  keyword_sentiment\n",
       "175           Worst Experience at Lo Stella              -0.60\n",
       "387  Restaurant's Added Fees are Ridiculous              -0.60\n",
       "391              Chipotle or Illegal Pete’s              -0.75\n",
       "412                          Missing Donuts              -1.00\n",
       "413                                   Stuck              -2.00"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sorted_file = file.sort_values(by = 'keyword_sentiment', ascending = False )[['title','keyword_sentiment']]\n",
    "sorted_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "702d2aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199                         Best paella?\n",
       "146            best saturday happy hour?\n",
       "232           Best Happy Hour in Denver?\n",
       "329    Best Monday Night Nice Restaurant\n",
       "201       Best Kid-Friendly restaurants?\n",
       "29          Best Dessert Recommendations\n",
       "113                    Best lunch sushi?\n",
       "114                     Best Malai Kofta\n",
       "9                     Best BRUNCH Denver\n",
       "142               Best Negroni in Denver\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the text for some of the highest and lowest sentiment scores\n",
    "\n",
    "sorted_file['title'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38754289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315    Can't miss food recommendations for a visiting...\n",
       "162                Place to go alone and possibly read? \n",
       "361    Plans to demolish Boulder's Dark Horse Tavern ...\n",
       "258                                  Grease Trap Service\n",
       "259         Hmm.. Why does pizza never get boring to me?\n",
       "175                        Worst Experience at Lo Stella\n",
       "387               Restaurant's Added Fees are Ridiculous\n",
       "391                           Chipotle or Illegal Pete’s\n",
       "412                                       Missing Donuts\n",
       "413                                                Stuck\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_file['title'].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecebed57",
   "metadata": {},
   "source": [
    "## Method 2: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf404e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/hanamengistu/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/hanamengistu/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/hanamengistu/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/hanamengistu/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/hanamengistu/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d13b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/hanamengistu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "# credit: https://www.datacamp.com/tutorial/text-analytics-beginners-nltk\n",
    "import nltk\n",
    "#nltk.download('all')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10621659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text function: cleans text to be easily processed for analysis\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    tokens = word_tokenize(text.lower()) # split the text into individual words and punctuation marks \n",
    "    \n",
    "    #remove stop words\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords.words('english'):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "            \n",
    "            \n",
    "    # Lemmatize the tokens \n",
    "    # Ex: jummping --> jump (stemming) \n",
    "    # Ex: run,ran, running --> lemma \"run\" (Lemmatize)\n",
    "    # reduce word to their root from \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    for token in filtered_tokens:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "        \n",
    "    \n",
    "    # join the tokens back into a string after being seprated in tokens \n",
    "    \n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcdd1e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>living denver little year - 's list</td>\n",
       "      <td>/r/denverfood/comments/1b6fom5/been_living_in_...</td>\n",
       "      <td>allanmuffins</td>\n",
       "      <td>174</td>\n",
       "      <td>You love Asian food OP haha. Only comment is h...</td>\n",
       "      <td>234</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue pan pizza</td>\n",
       "      <td>/r/denverfood/comments/1b62ip7/blue_pan_pizza/</td>\n",
       "      <td>JohnJAram</td>\n",
       "      <td>52</td>\n",
       "      <td>One of the owners played football at Michigan ...</td>\n",
       "      <td>103</td>\n",
       "      <td>Today was my son’s 19th birthday and based on ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>madras cafe closing month . 's next best place...</td>\n",
       "      <td>/r/denverfood/comments/1b6hgy7/madras_cafe_is_...</td>\n",
       "      <td>PlasmaWhore</td>\n",
       "      <td>9</td>\n",
       "      <td>Oh this is heartbreaking!  I don't know how an...</td>\n",
       "      <td>10</td>\n",
       "      <td>Just learned that Madras Cafe is closing. This...</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretty cool see :</td>\n",
       "      <td>/r/denverfood/comments/1b60ubw/this_was_pretty...</td>\n",
       "      <td>Namaste4Runner420</td>\n",
       "      <td>25</td>\n",
       "      <td>I guess it’s cool that Historian’s is finally ...</td>\n",
       "      <td>124</td>\n",
       "      <td></td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slovenian sausage and/or deli ?</td>\n",
       "      <td>/r/denverfood/comments/1b6hq3q/slovenian_sausa...</td>\n",
       "      <td>Likeabalrog</td>\n",
       "      <td>2</td>\n",
       "      <td>Cracovia in Westminster has a sausage sampler ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Are there any delis in the Denver area that se...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>birthday dinner recommendation</td>\n",
       "      <td>/r/denverfood/comments/198cml9/birthday_dinner...</td>\n",
       "      <td>thesnowgirl147</td>\n",
       "      <td>12</td>\n",
       "      <td>I had my birthday dinner at a the Wolf’s Tailo...</td>\n",
       "      <td>0</td>\n",
       "      <td>I am looking to have a more chill birthday thi...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>place sell vietnamese salt coffee ?</td>\n",
       "      <td>/r/denverfood/comments/197l8qe/is_there_a_plac...</td>\n",
       "      <td>DougDimmadummy</td>\n",
       "      <td>11</td>\n",
       "      <td>I don’t know for sure if its what your looking...</td>\n",
       "      <td>18</td>\n",
       "      <td>Same as the title says. I’m looking for a plac...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>find nduja</td>\n",
       "      <td>/r/denverfood/comments/197sb95/where_to_find_n...</td>\n",
       "      <td>broccoli15</td>\n",
       "      <td>5</td>\n",
       "      <td>Any well stocked gourmet market should have it...</td>\n",
       "      <td>7</td>\n",
       "      <td>Like the title says. Looking to find Nduja ide...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>’ best dumpling house denver/aurora ?</td>\n",
       "      <td>/r/denverfood/comments/197dryi/whats_the_best_...</td>\n",
       "      <td>Mysterious-Monk4349</td>\n",
       "      <td>37</td>\n",
       "      <td>I love Nana's and the owner owns a few in Auro...</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>psa denver vegan</td>\n",
       "      <td>/r/denverfood/comments/196w6tt/psa_for_denver_...</td>\n",
       "      <td>spicy_vegan69</td>\n",
       "      <td>302</td>\n",
       "      <td>Have any evidence or articles or is this a per...</td>\n",
       "      <td>337</td>\n",
       "      <td>Vegan Van is doing their 4th (or 5th?) Gofundm...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                  living denver little year - 's list   \n",
       "1                                       blue pan pizza   \n",
       "2    madras cafe closing month . 's next best place...   \n",
       "3                                    pretty cool see :   \n",
       "4                      slovenian sausage and/or deli ?   \n",
       "..                                                 ...   \n",
       "424                     birthday dinner recommendation   \n",
       "425                place sell vietnamese salt coffee ?   \n",
       "426                                         find nduja   \n",
       "427              ’ best dumpling house denver/aurora ?   \n",
       "428                                   psa denver vegan   \n",
       "\n",
       "                                                  link               author  \\\n",
       "0    /r/denverfood/comments/1b6fom5/been_living_in_...         allanmuffins   \n",
       "1       /r/denverfood/comments/1b62ip7/blue_pan_pizza/            JohnJAram   \n",
       "2    /r/denverfood/comments/1b6hgy7/madras_cafe_is_...          PlasmaWhore   \n",
       "3    /r/denverfood/comments/1b60ubw/this_was_pretty...    Namaste4Runner420   \n",
       "4    /r/denverfood/comments/1b6hq3q/slovenian_sausa...          Likeabalrog   \n",
       "..                                                 ...                  ...   \n",
       "424  /r/denverfood/comments/198cml9/birthday_dinner...       thesnowgirl147   \n",
       "425  /r/denverfood/comments/197l8qe/is_there_a_plac...       DougDimmadummy   \n",
       "426  /r/denverfood/comments/197sb95/where_to_find_n...           broccoli15   \n",
       "427  /r/denverfood/comments/197dryi/whats_the_best_...  Mysterious-Monk4349   \n",
       "428  /r/denverfood/comments/196w6tt/psa_for_denver_...        spicy_vegan69   \n",
       "\n",
       "     total_comments                                           comments  score  \\\n",
       "0               174  You love Asian food OP haha. Only comment is h...    234   \n",
       "1                52  One of the owners played football at Michigan ...    103   \n",
       "2                 9  Oh this is heartbreaking!  I don't know how an...     10   \n",
       "3                25  I guess it’s cool that Historian’s is finally ...    124   \n",
       "4                 2  Cracovia in Westminster has a sausage sampler ...      5   \n",
       "..              ...                                                ...    ...   \n",
       "424              12  I had my birthday dinner at a the Wolf’s Tailo...      0   \n",
       "425              11  I don’t know for sure if its what your looking...     18   \n",
       "426               5  Any well stocked gourmet market should have it...      7   \n",
       "427              37  I love Nana's and the owner owns a few in Auro...     24   \n",
       "428             302  Have any evidence or articles or is this a per...    337   \n",
       "\n",
       "                                                  text  keyword_sentiment  \n",
       "0                                                                0.000000  \n",
       "1    Today was my son’s 19th birthday and based on ...           0.000000  \n",
       "2    Just learned that Madras Cafe is closing. This...           0.230769  \n",
       "3                                                                0.333333  \n",
       "4    Are there any delis in the Denver area that se...           0.000000  \n",
       "..                                                 ...                ...  \n",
       "424  I am looking to have a more chill birthday thi...           0.000000  \n",
       "425  Same as the title says. I’m looking for a plac...           0.000000  \n",
       "426  Like the title says. Looking to find Nduja ide...           0.000000  \n",
       "427                                                              0.428571  \n",
       "428  Vegan Van is doing their 4th (or 5th?) Gofundm...           0.000000  \n",
       "\n",
       "[429 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['title'] = file['title'].apply(preprocess_text)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a3701d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword_sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>living denver little year - 's list</td>\n",
       "      <td>/r/denverfood/comments/1b6fom5/been_living_in_...</td>\n",
       "      <td>allanmuffins</td>\n",
       "      <td>174</td>\n",
       "      <td>You love Asian food OP haha. Only comment is h...</td>\n",
       "      <td>234</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue pan pizza</td>\n",
       "      <td>/r/denverfood/comments/1b62ip7/blue_pan_pizza/</td>\n",
       "      <td>JohnJAram</td>\n",
       "      <td>52</td>\n",
       "      <td>One of the owners played football at Michigan ...</td>\n",
       "      <td>103</td>\n",
       "      <td>Today was my son’s 19th birthday and based on ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>madras cafe closing month . 's next best place...</td>\n",
       "      <td>/r/denverfood/comments/1b6hgy7/madras_cafe_is_...</td>\n",
       "      <td>PlasmaWhore</td>\n",
       "      <td>9</td>\n",
       "      <td>Oh this is heartbreaking!  I don't know how an...</td>\n",
       "      <td>10</td>\n",
       "      <td>Just learned that Madras Cafe is closing. This...</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretty cool see :</td>\n",
       "      <td>/r/denverfood/comments/1b60ubw/this_was_pretty...</td>\n",
       "      <td>Namaste4Runner420</td>\n",
       "      <td>25</td>\n",
       "      <td>I guess it’s cool that Historian’s is finally ...</td>\n",
       "      <td>124</td>\n",
       "      <td></td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slovenian sausage and/or deli ?</td>\n",
       "      <td>/r/denverfood/comments/1b6hq3q/slovenian_sausa...</td>\n",
       "      <td>Likeabalrog</td>\n",
       "      <td>2</td>\n",
       "      <td>Cracovia in Westminster has a sausage sampler ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Are there any delis in the Denver area that se...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>birthday dinner recommendation</td>\n",
       "      <td>/r/denverfood/comments/198cml9/birthday_dinner...</td>\n",
       "      <td>thesnowgirl147</td>\n",
       "      <td>12</td>\n",
       "      <td>I had my birthday dinner at a the Wolf’s Tailo...</td>\n",
       "      <td>0</td>\n",
       "      <td>I am looking to have a more chill birthday thi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>place sell vietnamese salt coffee ?</td>\n",
       "      <td>/r/denverfood/comments/197l8qe/is_there_a_plac...</td>\n",
       "      <td>DougDimmadummy</td>\n",
       "      <td>11</td>\n",
       "      <td>I don’t know for sure if its what your looking...</td>\n",
       "      <td>18</td>\n",
       "      <td>Same as the title says. I’m looking for a plac...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>find nduja</td>\n",
       "      <td>/r/denverfood/comments/197sb95/where_to_find_n...</td>\n",
       "      <td>broccoli15</td>\n",
       "      <td>5</td>\n",
       "      <td>Any well stocked gourmet market should have it...</td>\n",
       "      <td>7</td>\n",
       "      <td>Like the title says. Looking to find Nduja ide...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>’ best dumpling house denver/aurora ?</td>\n",
       "      <td>/r/denverfood/comments/197dryi/whats_the_best_...</td>\n",
       "      <td>Mysterious-Monk4349</td>\n",
       "      <td>37</td>\n",
       "      <td>I love Nana's and the owner owns a few in Auro...</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.6808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>psa denver vegan</td>\n",
       "      <td>/r/denverfood/comments/196w6tt/psa_for_denver_...</td>\n",
       "      <td>spicy_vegan69</td>\n",
       "      <td>302</td>\n",
       "      <td>Have any evidence or articles or is this a per...</td>\n",
       "      <td>337</td>\n",
       "      <td>Vegan Van is doing their 4th (or 5th?) Gofundm...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                  living denver little year - 's list   \n",
       "1                                       blue pan pizza   \n",
       "2    madras cafe closing month . 's next best place...   \n",
       "3                                    pretty cool see :   \n",
       "4                      slovenian sausage and/or deli ?   \n",
       "..                                                 ...   \n",
       "424                     birthday dinner recommendation   \n",
       "425                place sell vietnamese salt coffee ?   \n",
       "426                                         find nduja   \n",
       "427              ’ best dumpling house denver/aurora ?   \n",
       "428                                   psa denver vegan   \n",
       "\n",
       "                                                  link               author  \\\n",
       "0    /r/denverfood/comments/1b6fom5/been_living_in_...         allanmuffins   \n",
       "1       /r/denverfood/comments/1b62ip7/blue_pan_pizza/            JohnJAram   \n",
       "2    /r/denverfood/comments/1b6hgy7/madras_cafe_is_...          PlasmaWhore   \n",
       "3    /r/denverfood/comments/1b60ubw/this_was_pretty...    Namaste4Runner420   \n",
       "4    /r/denverfood/comments/1b6hq3q/slovenian_sausa...          Likeabalrog   \n",
       "..                                                 ...                  ...   \n",
       "424  /r/denverfood/comments/198cml9/birthday_dinner...       thesnowgirl147   \n",
       "425  /r/denverfood/comments/197l8qe/is_there_a_plac...       DougDimmadummy   \n",
       "426  /r/denverfood/comments/197sb95/where_to_find_n...           broccoli15   \n",
       "427  /r/denverfood/comments/197dryi/whats_the_best_...  Mysterious-Monk4349   \n",
       "428  /r/denverfood/comments/196w6tt/psa_for_denver_...        spicy_vegan69   \n",
       "\n",
       "     total_comments                                           comments  score  \\\n",
       "0               174  You love Asian food OP haha. Only comment is h...    234   \n",
       "1                52  One of the owners played football at Michigan ...    103   \n",
       "2                 9  Oh this is heartbreaking!  I don't know how an...     10   \n",
       "3                25  I guess it’s cool that Historian’s is finally ...    124   \n",
       "4                 2  Cracovia in Westminster has a sausage sampler ...      5   \n",
       "..              ...                                                ...    ...   \n",
       "424              12  I had my birthday dinner at a the Wolf’s Tailo...      0   \n",
       "425              11  I don’t know for sure if its what your looking...     18   \n",
       "426               5  Any well stocked gourmet market should have it...      7   \n",
       "427              37  I love Nana's and the owner owns a few in Auro...     24   \n",
       "428             302  Have any evidence or articles or is this a per...    337   \n",
       "\n",
       "                                                  text  keyword_sentiment  \\\n",
       "0                                                                0.000000   \n",
       "1    Today was my son’s 19th birthday and based on ...           0.000000   \n",
       "2    Just learned that Madras Cafe is closing. This...           0.230769   \n",
       "3                                                                0.333333   \n",
       "4    Are there any delis in the Denver area that se...           0.000000   \n",
       "..                                                 ...                ...   \n",
       "424  I am looking to have a more chill birthday thi...           0.000000   \n",
       "425  Same as the title says. I’m looking for a plac...           0.000000   \n",
       "426  Like the title says. Looking to find Nduja ide...           0.000000   \n",
       "427                                                              0.428571   \n",
       "428  Vegan Van is doing their 4th (or 5th?) Gofundm...           0.000000   \n",
       "\n",
       "     sentiment  \n",
       "0       0.0000  \n",
       "1       0.0000  \n",
       "2       0.6369  \n",
       "3       0.6705  \n",
       "4       0.0000  \n",
       "..         ...  \n",
       "424     0.0000  \n",
       "425     0.0000  \n",
       "426     0.0000  \n",
       "427     0.6808  \n",
       "428     0.0000  \n",
       "\n",
       "[429 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize NLTK sentiment analyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# get sentiment function\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = scores['compound'] # gives decimal value \n",
    "    #if scores['pos'] > 0:\n",
    "        #sentiment = 1\n",
    "    #else:\n",
    "        #sentiment = 0\n",
    "\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "\n",
    "file['sentiment'] = file['title'].apply(get_sentiment)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe3bbb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>plan demolish boulder 's dark horse tavern dra...</td>\n",
       "      <td>-0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>go laptop book feel stupid near speer/cap hill...</td>\n",
       "      <td>-0.5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>chipotle illegal pete ’</td>\n",
       "      <td>-0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>worst experience lo stella</td>\n",
       "      <td>-0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>til 2020 , diner denver , colorado `` stupid q...</td>\n",
       "      <td>-0.7783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  sentiment\n",
       "361  plan demolish boulder 's dark horse tavern dra...    -0.5106\n",
       "354  go laptop book feel stupid near speer/cap hill...    -0.5267\n",
       "391                            chipotle illegal pete ’    -0.5574\n",
       "175                         worst experience lo stella    -0.6249\n",
       "68   til 2020 , diner denver , colorado `` stupid q...    -0.7783"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_file_new = file.sort_values(by = 'sentiment', ascending = False )[['title','sentiment']]\n",
    "sorted_file_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2d8e4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402    found great wall . huge box oyster mushroom $ ...\n",
       "232                             best happy hour denver ?\n",
       "146                           best saturday happy hour ?\n",
       "230    cool festival de bichos la diabla anyone inter...\n",
       "223    sweet sourdough park hill . 're awesome ( food...\n",
       "177    best mofongo denver ? willing drive suburb goo...\n",
       "374           best place get good cocktail watching game\n",
       "329                    best monday night nice restaurant\n",
       "277                  best special occasion brunch denver\n",
       "126                            best ayce daily special ?\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_file_new['title'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "348ab8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258                                  grease trap service\n",
       "387                   restaurant 's added fee ridiculous\n",
       "109                     lead fool gold sandwich denver ?\n",
       "124    sad taco tequila whiskey closing city park loc...\n",
       "326          jovanina ’ broken italian quality italian ?\n",
       "361    plan demolish boulder 's dark horse tavern dra...\n",
       "354    go laptop book feel stupid near speer/cap hill...\n",
       "391                              chipotle illegal pete ’\n",
       "175                           worst experience lo stella\n",
       "68     til 2020 , diner denver , colorado `` stupid q...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_file_new['title'].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f9eda",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6282c",
   "metadata": {},
   "source": [
    "Write a short summary of what you did and the results here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa0dc3",
   "metadata": {},
   "source": [
    "For this assignment, I began by retrieving my previous data using the sqlite3 library and executing MySQL commands. Once I had obtained my data, I commenced my sentiment analysis using Method 1, which involved the keyword approach.\n",
    "\n",
    "In the keyword method, I utilized the AFINN-en-165.txt file provided for sentiment analysis. After downloading the file, I converted the DataFrame into a dictionary format. Subsequently, I proceeded to obtain the sentiment value for each title in my Reddit data. The process involved a loop that performed the following tasks: converting the title to lowercase format, splitting each word, and employing a nested loop to search for words from the Reddit titles in the sentiment word dictionary I had previously created. If the nested loop found the word in the dictionary, it appended the word along with its corresponding sentiment value into an array. If the word was not found, it was given a value of 0. Finally, all the sentiment values were stored in an array and added to a column within the Reddit DataFrame. Following this, I created a histogram plot of the sentiment values and provided a descriptive analysis of the sentiment values, which included statistical insights. Additionally, I examined the top 10 titles with the highest sentiment scores and the bottom 10 with the lowest sentiment scores.\n",
    "\n",
    "Next, I conducted sentiment analysis using the NLTK method. When employing NLTK, I initially utilized a function called preprocess_text, which converted the provided text to lowercase, removed stopwords such as \"the\" and \"and\", and lemmatized the words. Essentially, this function cleaned the titles of the Reddit posts by eliminating unnecessary words and punctuations. Subsequently, I employed a function called get_sentiment, which utilized the predefined machine learning model analyzer = SentimentIntensityAnalyzer(). The sentiment score of the title was obtained using the code scores = analyzer.polarity_scores(text), and for this function, we utilized the compound score as it provided a comprehensive view of the sentiment value. After applying these functions to the entire dataset, it was observed that the descriptive statistics of the sentiment values differed from Method 1. Moreover, the top 10 and bottom 10 titles also differed from those obtained through Method 1.\n",
    "\n",
    "There is a diffrent outcomes between the two methods do to the fact that the compound score does not represent the mean sentiment, unlike in the first method where it was calculated. Additionally, I believe that the divergent outcomes of the two methods may be due to the removal of stopwords and lemmatization of words in the NLTK method.In my opinion the NLTK method provides a more accurate analysis compared to the keyword method. One notable observation regarding the top 10 titles with high sentiment values versus the bottom 10 is the presence of positive words such as \"best,\" \"great,\" and \"sweet\" in the former, whereas the latter contains words like \"trap,\" \"worst,\" \"missing,\" and \"stuck,\" which typically carry negative connotations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
